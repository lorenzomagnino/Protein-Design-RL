Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Training PPO on Protein-Design-v0 for 50000 timesteps...
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 0.187    |
| time/              |          |
|    fps             | 991      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 1.04       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 2          |
|    time_elapsed         | 4          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.01574023 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.98      |
|    explained_variance   | -0.0747    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.031     |
|    value_loss           | 0.96       |
----------------------------------------
Eval num_timesteps=5000, episode_reward=7.20 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 7.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012738159 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.95       |
|    explained_variance   | 0.0645      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.929       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 1.97        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 2          |
| time/                   |            |
|    fps                  | 801        |
|    iterations           | 4          |
|    time_elapsed         | 10         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01340114 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.92      |
|    explained_variance   | 0.145      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.468      |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 1.41       |
----------------------------------------
Eval num_timesteps=10000, episode_reward=1.50 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012080803 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.144       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.46        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 2.7         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 792      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 3.96        |
| time/                   |             |
|    fps                  | 788         |
|    iterations           | 6           |
|    time_elapsed         | 15          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011268616 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 3.2         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 4.76       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 7          |
|    time_elapsed         | 18         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01288923 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.245      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 4.57       |
----------------------------------------
Eval num_timesteps=15000, episode_reward=7.90 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 7.9         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013156911 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.92        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 5.17        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 781         |
|    iterations           | 9           |
|    time_elapsed         | 23          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.013988691 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.45       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.86        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 5.91        |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=14.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010901665 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.51        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 5.41        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 8.99     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 10.1        |
| time/                   |             |
|    fps                  | 778         |
|    iterations           | 11          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012172548 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 4.16        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 11.1        |
| time/                   |             |
|    fps                  | 777         |
|    iterations           | 12          |
|    time_elapsed         | 31          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016746048 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 2.55        |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=13.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015204865 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 1.88        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 13       |
|    time_elapsed    | 34       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 12.6        |
| time/                   |             |
|    fps                  | 775         |
|    iterations           | 14          |
|    time_elapsed         | 36          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.018093208 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0388     |
|    value_loss           | 1.5         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=13.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.023887498 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0075     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0465     |
|    value_loss           | 0.322       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 15       |
|    time_elapsed    | 39       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 13.1        |
| time/                   |             |
|    fps                  | 774         |
|    iterations           | 16          |
|    time_elapsed         | 42          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011867402 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.167       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.592       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 774        |
|    iterations           | 17         |
|    time_elapsed         | 44         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.01447648 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0112    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=13.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012950563 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.0645      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 773      |
|    iterations      | 18       |
|    time_elapsed    | 47       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 13.2        |
| time/                   |             |
|    fps                  | 773         |
|    iterations           | 19          |
|    time_elapsed         | 50          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.014718712 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.0533      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=13.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.012329526 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.0577      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 20       |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 13.2       |
| time/                   |            |
|    fps                  | 772        |
|    iterations           | 21         |
|    time_elapsed         | 55         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.01193887 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0122    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0331    |
|    value_loss           | 0.0715     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=13.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013322066 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00419    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.0844      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 22       |
|    time_elapsed    | 58       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 13.2        |
| time/                   |             |
|    fps                  | 772         |
|    iterations           | 23          |
|    time_elapsed         | 61          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.011206154 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00709    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.0822      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 13.4        |
| time/                   |             |
|    fps                  | 772         |
|    iterations           | 24          |
|    time_elapsed         | 63          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010415303 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00145    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 0.068       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=14.10 +/- 0.00
Episode length: 15.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012732177 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0231      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.0908      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 771      |
|    iterations      | 25       |
|    time_elapsed    | 66       |
|    total_timesteps | 51200    |
---------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 51,200/50,000  [ 0:01:05 < 0:00:00 , 788 it/s ]
Model saved at /gpfsnyu/home/lm5489/Protein-Design-RL/saved-results/model_saved/PPO_Protein_Design
